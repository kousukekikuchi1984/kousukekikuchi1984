# 基本情報

| 項目名   | 内容                                                 |
| -------- | ---------------------------------------------------- |
| 名前     | 菊地 弘祐（きくち こうすけ）                |
| 生年月   | 1984年3月 |
| 居住地   | 兵庫県西宮市 |
| 最終学歴 | 早稲田大学大学院創造理工学研究科建築学専攻博士課程単位取得退学 |

# 職務要約

- 誤ったデータを登録することを許容しない設計・実装・思想
  - ミッションクリティカルな領域
    - 会計データ
    - ポイントシステム
  - 性能よりも安全性
    - 速度改善よりもデータ整合性
    - 短期的な速度改善や局所改善が将来的な破綻や説明不能な状態を生むと判断した場合は、実装を止める判断をしてきた
  - 後から正しさを追試できる構造


# 得意領域・技術スタック

## ミッションクリティカル領域のデータ整合性設計
- 会計データ／ポイント残高など、誤りが確定すると修正が困難な領域の設計・実装
- 冪等性、状態遷移、境界条件、再実行戦略を含めた「壊れない」設計
- 後から正しさを追試・説明できるデータモデル／処理構造の設計

## 長期運用を前提としたバックエンドアーキテクチャ
- 保守性・再利用性を重視した設計（ドメインモデル、クリーンアーキテクチャ等）
- 性能最適化よりも安全性を優先すべき局面の判断（止める判断を含む）
- レガシー構成からモダン構成への段階的移行

## データ基盤・バッチ処理の運用設計
- バッチ処理／集計／検証の設計・運用（運用要件、異常検知、復旧を含む）
- ワークフロー（リトライ、通知、確定ルール）を含めた運用負荷の最適化

## 横断的な実装経験
- バックエンドを主軸としつつ、TypeScript + Vue.js によるフロントエンド開発経験
- UX／性能改善を目的としたフロント・バック両面での調整経験


# 経歴
## Engineer（YUZURIHA）

2021.01-現在 在籍

### 役割概要
ミッションクリティカルなデータを扱うシステムにおいて、誤ったデータが正として確定しない構造の設計・判断を主責務として担当し技術面の意思決定やレビュー方針の策定も担った。

### 代表的な取り組み
#### プロダクト立ち上げ期における会計データ出力の不備
##### 課題
会計システムが利用する CSV 出力について、特定条件下で正しく計上されない不具合が潜在していることを発見した。
当該 CSV は 月次集計に利用され、外部会計システムへ連携されるほか、複数部署が参照するデータであり、リリース後に不具合が発覚した場合、会計データの修正・再計上に多大なコストが発生するリスクがあった。

##### 判断・行動

- 仕様と実装を突き合わせて検証した結果、ポイントの有効期限失効時に、会計上正しく計上されないケースが存在することを特定
- 計上ルール自体は正しい一方で、そのルールを前提とした設計になっていなかったことが根本原因であると判断
- 一時的な補正ではなく、計上ルールに合致する形へ設計を是正し、修正を実施

##### 結果

- 月次・外部会計・複数部署参照という条件下で、会計データが誤った形で確定する事態をリリース前に未然防止
- 「システム上は正常だが、会計としては壊れている」状態をリリース前に排除

#### クレジットカード会社との大規模データ連携（100万件／時間制約）
##### 課題
クレジットカード会社とのデータ連携において、100万件のデータを1時間以内に安全に取り込むという制約があった。
誤投入や二重投入が発生すると、利用者情報やポイント残高が誤ったまま確定し、後追い修正が困難になるリスクがあった。

##### 判断・行動
- 外部仕様がデータ番号の昇順に適用していけば良いという方針を先方から受け取ったが、そもそもの例外が多く、データ投入単位でまとまっていないため、そもそものルールの再定義を行った
- 並列処理の安全単位を確定するため、受領レコード単位（カード単位）からユーザー単位のキーを特定し、投入単位・ロック粒度・補正ロジックを設計した
- 例外条件を外部と突き合わせ、矛盾が生じるケースをパターン化して合意し、連携仕様を“運用可能なルール”として確定させた
- 大量取り込みと誤投入防止がトレードオフになるため、Validation と投入を分離し、投入条件を厳格化する判断をした
- またデータ投入時にエラーが発生することも考え冪等性を担保し、再実行時にもデータが壊れない構造を設計
- 取り込み途中でアカウント統合が発生する可能性を考慮し、ロックによって状態を確定させた上で投入（状態変更があった場合は適切にデータを補正）
- 結果として安全で実測で約6分での取り込みを実現

##### 結果
- 高速かつ安全なデータ連携を実現
- 運用・再実行・障害時にも正しさを説明できる構造を確立


#### 性能改善要求に対する設計判断（速度改善を止める判断）
##### 課題
性能改善要請があり特に現状調査も行われず同僚の案で進むことになった。ただ、現行構造のまま最適化を進めると、将来的にデータ整合性を破壊するリスクが高い状況だった。
この案を進めた最悪の場合、ポイント残高が誤ったまま確定し、取引履歴の再計算・復旧ロジックが二重化して原因追跡も困難になる懸念があった。

##### 判断・行動
- これはポイント残高計算のロジックの根幹箇所で変更されなくなったデータを確定箇所であるが、そもそも実装難易度が高く理解できている人が限られている状態であった
- 実行が求められた primitive 化などの局所最適は、Python では cache locality があたらないため効果が低いどころか復旧ロジックの二重化・理解困難化を招くと判断
- 調査の結果ボトルネックがアプリケーションではなくMySQL の commit 時 fsync に待ちがあることを特定
- SWE としての実装継続では解決しないと結論づけ、現状のボトルネック調査した上でcommit 数削減や責務分離といった代替案を提示した上でタスクを PM に差し戻し

##### 結果
- 長期的に破綻する可能性のある設計分岐を回避
- データ整合性を最優先とする判断を組織として合意

### レビュー・育成における取り組み

 - チーム内のコミュニケーション量を増やすため、業務外も含めた対話の機会を意図的に設計
 - レビューでは、単なる指摘に留めず、判断の背景や考え方を必ず言語化して共有
 - また、デプロイ時や非同期処理の境界でエラーやデータが壊れることがないかを重点的に指摘
 - 感情的な指導や叱責は行わず、心理的安全性を前提としたフィードバックを重視
 - 1on1 では、本人のスキルや状況を踏まえ、最も伸びる一点に絞って具体的な改善点を提示
 - 日本語表現やドキュメントの書き方など、成果物の品質に直結する基礎スキルの支援も実施

## Data Reliability Engineer（Gunosy）

2019.05-2020.12 在籍

### 役割概要
大規模データ基盤において、アプリケーション・データ移行・集計・ビジネス運用を横断し、データの正当性・再現性・説明可能性を担保。

### 代表的な取り組み

#### SSP 収益データ取得・確定ワークフロー設計

##### 業務内容
各メディアが利用する SSP の収益データを取得し、AWS Athena で分析可能な形に整備。最終的にはビジネスサイドで記事の配信元と按分する。

##### 判断・行動
- 収益発生日から60時間後に速報値を確定し、72時間後に数字を確定しこれ以降は変更しないという特殊仕様に対応
- Digdag（Workflow Engine）の想定外の挙動に直面しながらも、実装をブラックボックス化せずコードを精査して原因を特定・解決

##### 成果
- ビジネス要件とデータ整合性を両立したワークフローを構築
- 予期せぬ挙動に対しても再現性をもって対処できる基盤を確立
  - 参考： https://tech.gunosy.io/entry/escape-from-pitfall
- この際にビジネスで収益按分割合の誤りを指摘

#### データ基盤民主化に向けたワークフローテンプレート作成

##### 業務内容
特定チームのみが利用していたワークフローエンジンを、他チームでも安全に使えるようにするためのテンプレートを設計。

##### 判断・行動

- bash による最低限の入力で利用可能なテンプレートを作成
- 利用者側と基盤側の責任分解点を明確に定義

##### 成果
- ワークフロー作成・運用コストを大幅に削減
- 基盤チーム・利用チーム双方の負担を軽減

#### 運用負荷低減を目的としたワークフロー整理

##### 業務内容
エラー通知が「データ欠損防止」という観点で設計されておらず、不要な運用負荷が発生していた。

##### 判断・行動
- 時間経過により自然に正しい状態へ収束するパターンを分類
- 入力・出力別に「次セッションで必ず正しくなる」構造へワークフローを再設計

##### 成果
- データ正当性を維持しつつ、運用負荷を体系的に削減
- 障害対応を“人の判断”から“構造”へ移行


### システムエンジニア（パーソルキャリア）

2017.10-2019.04 在籍

- 求人・転職領域の大規模Webサービスおよびデータ基盤関連プロジェクトに従事
- 既存システムの保守・改善を中心に、安定運用を前提としたバックエンド開発を担当
- データ基盤やバッチ処理における前提条件のずれや検証不足が、後工程で大きな修正コストを生む場面を多数経験
- この経験を通じて、実装そのものよりも「設計段階での判断」や「正しさを説明できる構造」の重要性を強く意識するようになった

### データマイニングエンジニア（株式会社IPオンウェブジャパン）

2016.02-2017.09 在籍

- 広告配信・収益データを対象とした分析および運用支援に従事
- ログや集計結果をもとに、数値の不整合や想定外の挙動を発見・原因特定する役割を担当
- 「システム上は正常に見えるが、データとしてはおかしい」状態を継続的に扱う
- データの変化や分布の違和感から異常を見抜く経験を積み、後工程での修正が困難になる前に問題を潰す姿勢が形成された

### エンジニア（株式会社Spotlight）

2014.04-2016.01 在籍

- レポート生成・表示処理の速度改善に取り組み、ユーザー体験と運用効率の向上を実現
- 特定の技術領域に閉じず、課題解決に必要な技術を分け隔てなく扱う姿勢で開発に従事
- この経験を通じて、現在のフルスタックな視点と、技術選定を手段として捉える考え方の基礎が形成された
